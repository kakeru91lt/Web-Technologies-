{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDsXPH6TpCwJG6TpN7HPeo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kakeru91lt/Web-Technologies-/blob/main/QA_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "95jWrNgWRdp8",
        "outputId": "eb8529f9-cc8d-4d3e-f3bf-6e99a267134b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement string (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for string\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install torch\n",
        "!pip install nltk\n",
        "!pip install string\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the notebook runs in Google Colab environment\n",
        "# If you're running this locally, install the required packages with:\n",
        "# !pip install torch transformers sklearn nltk pandas\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizerFast, BertForQuestionAnswering, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define function to remove links from text\n",
        "def remove_links(text):\n",
        "    pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "# Define function to remove stop words\n",
        "def remove_stop_words(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
        "\n",
        "# Define function to remove punctuations\n",
        "def remove_punctuations(text):\n",
        "    return ''.join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "# Define the Porter Stemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Define function to stem text\n",
        "def stem_text(text):\n",
        "    return ' '.join(ps.stem(word) for word in text.split())\n",
        "\n",
        "def tokenize_and_encode(questions, answers, tokenizer, max_len):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for question, answer in zip(questions, answers):\n",
        "        encoded_dict = tokenizer.encode_plus(question,\n",
        "                                             answer,\n",
        "                                             add_special_tokens=True,\n",
        "                                             max_length=max_len,\n",
        "                                             truncation=True,\n",
        "                                             padding='max_length',\n",
        "                                             return_attention_mask=True,\n",
        "                                             return_offsets_mapping=True)\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "        offsets = encoded_dict['offset_mapping']\n",
        "        start_idx, end_idx = None, None\n",
        "\n",
        "        for idx, (start, end) in enumerate(offsets):\n",
        "            if start_idx is None and start != 0 and question[start:end] in answer:\n",
        "                start_idx = idx\n",
        "            if start_idx is not None and question[start:end] in answer and end == len(answer):\n",
        "                end_idx = idx\n",
        "                break\n",
        "\n",
        "        if start_idx is None or end_idx is None:\n",
        "            print(\"Failed to find start or end positions for:\")\n",
        "            print(\"Question:\", question)\n",
        "            print(\"Answer:\", answer)\n",
        "            print(\"Offsets:\", offsets)\n",
        "            # Set start and end positions to -1 to indicate that answer is not present in the question\n",
        "            start_idx = -1\n",
        "            end_idx = -1\n",
        "\n",
        "        start_positions.append(start_idx)\n",
        "        end_positions.append(end_idx)\n",
        "\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "    start_positions = torch.tensor(start_positions)\n",
        "    end_positions = torch.tensor(end_positions)\n",
        "\n",
        "    assert len(input_ids) == len(attention_masks) == len(start_positions) == len(end_positions), \"Length mismatch between tensors\"\n",
        "\n",
        "    return input_ids, attention_masks, start_positions, end_positions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/qa.csv\") # Update with your dataset path\n",
        "\n",
        "# Apply preprocessing steps\n",
        "data['cleaned_question'] = data['question'].apply(remove_links)\n",
        "data['cleaned_question'] = data['cleaned_question'].apply(remove_stop_words)\n",
        "data['cleaned_question'] = data['cleaned_question'].apply(remove_punctuations)\n",
        "data['cleaned_question'] = data['cleaned_question'].apply(stem_text)\n",
        "\n",
        "data['cleaned_answer'] = data['answer'].apply(remove_links)\n",
        "data['cleaned_answer'] = data['cleaned_answer'].apply(remove_stop_words)\n",
        "data['cleaned_answer'] = data['cleaned_answer'].apply(remove_punctuations)\n",
        "data['cleaned_answer'] = data['cleaned_answer'].apply(stem_text)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "max_len = 128  # Adjust based on your dataset\n",
        "\n",
        "# Tokenize and encode data\n",
        "train_input_ids, train_attention_masks, train_start_positions, train_end_positions = tokenize_and_encode(\n",
        "    train_data['cleaned_question'].values, train_data['cleaned_answer'].values, tokenizer, max_len)\n",
        "val_input_ids, val_attention_masks, val_start_positions, val_end_positions = tokenize_and_encode(\n",
        "    val_data['cleaned_question'].values, val_data['cleaned_answer'].values, tokenizer, max_len)\n",
        "\n",
        "print(\"Size of train_input_ids:\", train_input_ids.size())\n",
        "print(\"Size of train_attention_masks:\", train_attention_masks.size())\n",
        "print(\"Size of train_start_positions:\", train_start_positions.size())\n",
        "print(\"Size of train_end_positions:\", train_end_positions.size())\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_start_positions, train_end_positions)\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_start_positions, val_end_positions)\n",
        "\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 8\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
        "\n",
        "# Initialize model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Define learning rate scheduler\n",
        "num_epochs = 3\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, attention_masks, start_positions, end_positions = batch\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_masks, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_start_preds = []\n",
        "    all_end_preds = []\n",
        "    all_start_labels = []\n",
        "    all_end_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, attention_masks, start_positions, end_positions = batch\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_masks)\n",
        "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "\n",
        "            start_preds = torch.argmax(start_logits, dim=1)\n",
        "            end_preds = torch.argmax(end_logits, dim=1)\n",
        "\n",
        "            all_start_preds.extend(start_preds.cpu().numpy())\n",
        "            all_end_preds.extend(end_preds.cpu().numpy())\n",
        "            all_start_labels.extend(start_positions.cpu().numpy())\n",
        "            all_end_labels.extend(end_positions.cpu().numpy())\n",
        "\n",
        "    start_accuracy = accuracy_score(all_start_labels, all_start_preds)\n",
        "    end_accuracy = accuracy_score(all_end_labels, all_end_preds)\n",
        "\n",
        "    start_precision = precision_score(all_start_labels, all_start_preds, average='weighted')\n",
        "    end_precision = precision_score(all_end_labels, all_end_preds, average='weighted')\n",
        "\n",
        "    start_recall = recall_score(all_start_labels, all_start_preds, average='weighted')\n",
        "    end_recall = recall_score(all_end_labels, all_end_preds, average='weighted')\n",
        "\n",
        "    start_f1 = f1_score(all_start_labels, all_start_preds, average='weighted')\n",
        "    end_f1 = f1_score(all_end_labels, all_end_preds, average='weighted')\n",
        "\n",
        "    return start_accuracy, end_accuracy, start_precision, end_precision, start_recall, end_recall, start_f1, end_f1\n",
        "\n",
        "# Training and evaluation loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_model(model, train_dataloader, optimizer, scheduler, device)\n",
        "    start_acc, end_acc, start_prec, end_prec, start_rec, end_rec, start_f1, end_f1 = evaluate_model(model, val_dataloader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "    print(f\"Validation Start Accuracy: {start_acc:.4f}, Precision: {start_prec:.4f}, Recall: {start_rec:.4f}, F1: {start_f1:.4f}\")\n",
        "    print(f\"Validation End Accuracy: {end_acc:.4f}, Precision: {end_prec:.4f}, Recall: {end_rec:.4f}, F1: {end_f1:.4f}\")\n",
        "\n",
        "# Save model\n",
        "model_save_path = './bert_qa_model.pth'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(\"Model saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "s4SL4efKcK5d",
        "outputId": "89428126-ebb9-4633-bf57-958abd3b2f09"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to find start or end positions for:\n",
            "Question: favorit featur\n",
            "Answer: person prefer\n",
            "Offsets: [(0, 0), (0, 5), (5, 7), (8, 12), (12, 14), (0, 0), (0, 6), (7, 13), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
            "Failed to find start or end positions for:\n",
            "Question: element repres symbol fe period tabl\n",
            "Answer: element repres symbol fe iron\n",
            "Offsets: [(0, 0), (0, 7), (8, 11), (11, 14), (15, 21), (22, 24), (25, 31), (32, 35), (35, 36), (0, 0), (0, 7), (8, 11), (11, 14), (15, 21), (22, 24), (25, 29), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
            "Failed to find start or end positions for:\n",
            "Question: element repres symbol fe period tabl\n",
            "Answer: element repres symbol fe iron\n",
            "Offsets: [(0, 0), (0, 7), (8, 11), (11, 14), (15, 21), (22, 24), (25, 31), (32, 35), (35, 36), (0, 0), (0, 7), (8, 11), (11, 14), (15, 21), (22, 24), (25, 29), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]\n",
            "Size of train_input_ids: torch.Size([1083, 128])\n",
            "Size of train_attention_masks: torch.Size([1083, 128])\n",
            "Size of train_start_positions: torch.Size([1083])\n",
            "Size of train_end_positions: torch.Size([1083])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "Training Loss: 0.8999\n",
            "Validation Start Accuracy: 0.9752, Precision: 0.9521, Recall: 0.9752, F1: 0.9633\n",
            "Validation End Accuracy: 0.9917, Precision: 0.9847, Recall: 0.9917, F1: 0.9879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3\n",
            "Training Loss: 0.0452\n",
            "Validation Start Accuracy: 0.9752, Precision: 0.9521, Recall: 0.9752, F1: 0.9633\n",
            "Validation End Accuracy: 0.9917, Precision: 0.9847, Recall: 0.9917, F1: 0.9879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3\n",
            "Training Loss: 0.0253\n",
            "Validation Start Accuracy: 0.9752, Precision: 0.9521, Recall: 0.9752, F1: 0.9633\n",
            "Validation End Accuracy: 0.9917, Precision: 0.9847, Recall: 0.9917, F1: 0.9879\n",
            "Model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('bert_qa_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AUDhEAnIh_ky",
        "outputId": "8b584a3e-521c-416f-e19c-40c3e1fbbdfe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fb387922-352a-4ff8-bea2-1044fd4eaec7\", \"bert_qa_model.pth\", 435654822)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}